{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Environment Description\n",
    "\n",
    "- No Obstructions in Unity Enviroment, just Bi-Pedal Agent & Reward Box \n",
    "- Thinking of adding randomly generated obstructions in later iterations (need add anything new in the Unity Design Engine on MacBook)\n",
    "- Python Environment is included in directory inside of \"/.env\" use `conda activate /.env` to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
      "[INFO] Connected new brain: Walker?team=0\n",
      "[WARNING] The environment contains multiple observations. You must define allow_multiple_obs=True to receive them all. Otherwise, only the first visual observation (or vector observation ifthere are no visual observations) will be provided in the observation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckg/miniconda3/envs/walker/lib/python3.9/site-packages/gym/logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "# Load in Custom Env -- View Head Mode\n",
    "import gym\n",
    "import time\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper\n",
    "\n",
    "#\"/Volumes/GoogleDrive/My Drive/Colab/IDS 576 - Deep Learning/Final Project/ml-agents/gym-unity/gym_unity/envs/Environment-1.app\" #Env's home during project\n",
    "#\"/Users/ckg-files/UnityTest/CustomUnityEnvironments/Environment-1.app\" #moved to git dir\n",
    "\n",
    "# This is the custom Unity Environment that we pass to the `Gym` Wrapper\n",
    "CUSTOM_ENV_PATH = \"CustomUnityEnvironments/Environment-1.x86_64\" #extension is \".app\" on MacOS\n",
    "unity_env = UnityEnvironment(CUSTOM_ENV_PATH, \n",
    "                             no_graphics=True) #no_graphics=True to run in headless mode\n",
    "# On Custom Unity Environments:\n",
    "#BaseEnvironment.app has no obstructions -- may add if we have time to teach agent to walk around\n",
    "#first env was: `CKG-Walker.app` -- reskin of the provided one (with a single agent, cant do multiple if using custom model)\n",
    "\n",
    "# Send to OpenAI Gym\n",
    "env = UnityToGymWrapper(unity_env, uint8_visual=False, flatten_branched=False, allow_multiple_obs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Env\n",
    "\n",
    "- Check the input & outputs of Actions/States in the user created env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Walker?team=0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Info about Env\n",
    "\n",
    "env.action_size #39 degrees of freedom (joints over which agency is given to the agent)\n",
    "env.action_space #Gym Like Description of Action_Space (from the Unity Env)\n",
    "\n",
    "env.observation_space #Input Vector of shape (243, ) -- 1D vector of length 243 (each val represents the Walker's body postion relative to the floor?)\n",
    "\n",
    "env.name #Walker is the name of the Env! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Shape: (39,) - float32\n",
      "Observation Space Shape: (243,) - float32\n",
      "\n",
      "\n",
      "Action Space: Box([-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1.], [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], (39,), float32)\n",
      "Observation Space: Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf], (243,), float32)\n"
     ]
    }
   ],
   "source": [
    "# Get Environment Params \n",
    "print(f\"Action Space Shape: {env.action_space.shape} - {env.action_space.dtype}\") #all actions are continuous but bounded between {-1 & 1} (i.e lower & upper bounds)\n",
    "print(f\"Observation Space Shape: {env.observation_space.shape} - {env.observation_space.dtype}\")\n",
    "#observation space is unbounded, could be any value {-inf to inf}\n",
    "\n",
    "env.action_size #39 degrees of freedom (joints over which agency is given to the agent)\n",
    "env.observation_space #Input Vector of shape (243, ) -- 1D vector of length 243\n",
    "\n",
    "env.name #Walker is the name of the Env! \n",
    "print(\"\\n\")\n",
    "\n",
    "# Actual Action & Observation Spaces\n",
    "print(\"Action Space:\", env.action_space) #all actions are discrete movements\n",
    "print(\"Observation Space:\", env.observation_space) #Continuous Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Description\n",
    "\n",
    "Chosen Model - Proximal Policy Optimization (PPO Continuous)\n",
    "\n",
    "RL model, this effectively means that for `on policy` learning the function that approximates the optimal policy learns from its actions as it takes them, an off-policy model would instead learn it's policy function by taking a series of random actions & mapping the value derived from them\n",
    "\n",
    "<BR>\n",
    "\n",
    "**References**\n",
    "\n",
    "[OpenAI Post on PPO](https://openai.com/blog/openai-baselines-ppo/)\n",
    "\n",
    "[On-Policy Vs. Off-Policy RL Models](https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning) \n",
    "\n",
    "[Upload PyTorch Model into Unity](https://medium.com/@a.abelhopereira/how-to-use-pytorch-models-in-unity-aa1e964d3374)\n",
    "\n",
    "[RL Overview - OpenAI](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)\n",
    "\n",
    "<BR>\n",
    "\n",
    "**Model Architecture**\n",
    "\n",
    "Input = [243, 1] -- What the network sees\n",
    "\n",
    "Output = [39, 1] -- Possible actions, \n",
    "\n",
    "Reward = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE PPO From minimalRL\n",
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from gym_unity.envs import UnityToGymWrapper\n",
    "\n",
    "\n",
    "# HyperParams -- Originals\n",
    "learning_rate  = 0.0003\n",
    "gamma          = 0.9\n",
    "lmbda          = 0.9\n",
    "eps_clip       = 0.2\n",
    "K_epoch        = 10\n",
    "rollout_len    = 3\n",
    "buffer_size    = 30\n",
    "minibatch_size = 32\n",
    "\n",
    "\n",
    "# UDFs\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        # Layers\n",
    "        self.fc1     = nn.Linear(243, 128) #input is observation space of len 243\n",
    "        self.fc_mu   = nn.Linear(128, 39) #need 39 as output for all `inbetween` layers, all feed into the output layer!\n",
    "        self.fc_std  = nn.Linear(128, 39)\n",
    "        self.fc_v    = nn.Linear(128, 39) #output determines our 39 joints movements\n",
    "        \n",
    "        # Original Layers\n",
    "        #self.fc1   = nn.Linear(3,128)\n",
    "        #self.fc_mu = nn.Linear(128,1)\n",
    "        #self.fc_std  = nn.Linear(128,1)\n",
    "        #self.fc_v = nn.Linear(128,1)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = 2.0 * torch.tanh(self.fc_mu(x))\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        return mu, std\n",
    "\n",
    "\n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "\n",
    "        for j in range(buffer_size):\n",
    "            for i in range(minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition\n",
    "\n",
    "                    # Original Implementation - Just Lists of Scalar Value or Lists of Scalars\n",
    "                    #s_lst.append(s)\n",
    "                    #a_lst.append([a])\n",
    "                    #r_lst.append([r])\n",
    "                    #s_prime_lst.append(s_prime)\n",
    "                    #prob_a_lst.append([prob_a])\n",
    "                    #done_mask = 0 if done else 1\n",
    "                    #done_lst.append([done_mask])\n",
    "                    \n",
    "                    # CKG Added - Convert to Tensors (still appending to lists? will try to change)\n",
    "                    #s_lst.append(torch.tensor(s, dtype=torch.float))\n",
    "                    #a_lst.append(torch.tensor(a, dtype=torch.float))\n",
    "                    #r_lst.append([r])\n",
    "                    #s_prime_lst.append(torch.tensor(s_prime, dtype=torch.float))\n",
    "                    #prob_a_lst.append(torch.tensor(prob_a, dtype=torch.float))\n",
    "                    #done_mask = 0 if done else 1\n",
    "                    #done_lst.append(torch.tensor(done_mask, dtype=torch.float))\n",
    "\n",
    "                    # 2nd Attempt -- convert all to tensors and pass to batch lists (iter through list and access tensors instead of lists of scalars -- in original)\n",
    "                    s = torch.tensor(s, dtype=torch.float)\n",
    "                    a = torch.tensor(a, dtype=torch.float)\n",
    "                    r = torch.tensor(r, dtype=torch.float) #scalar reward value!\n",
    "                    s_prime = torch.tensor(s_prime, dtype=torch.float)\n",
    "                    prob_a = torch.tensor(prob_a, dtype=torch.float)\n",
    "                    done_mask = 0 if done else 1\n",
    "                    done_mask = torch.tensor(done_mask, dtype=torch.float)\n",
    "\n",
    "                # CKG Version -- List of Tensors (should work with matmuls and is a list per batch!) -- WILL TRY APPENDING DIRECTLY TO MINI-BATCH STATEMENT\n",
    "                #s_batch.append(s)\n",
    "                #a_batch.append(a)\n",
    "                #r_batch.append(r)\n",
    "                #s_prime_batch.append(s_prime)\n",
    "                #prob_a_batch.append(prob_a)\n",
    "                #done_batch.append(done_mask)\n",
    "                \n",
    "                # Original Appending, lists of lists, CKG Version above\n",
    "                #s_batch.append(s_lst)\n",
    "                #a_batch.append(a_lst)\n",
    "                #r_batch.append(r_lst)\n",
    "                #s_prime_batch.append(s_prime_lst)\n",
    "                #prob_a_batch.append(prob_a_lst)\n",
    "                #done_batch.append(done_lst)\n",
    "\n",
    "            # Original - cast to tensor as end\n",
    "#            mini_batch =(torch.tensor(s_batch, dtype=torch.float), \n",
    "#                         torch.tensor(a_batch, dtype=torch.float), \\\n",
    "#                         torch.tensor(r_batch, dtype=torch.float), \\\n",
    "#                         torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "#                         torch.tensor(done_batch, dtype=torch.float), \\\n",
    "#                         torch.tensor(prob_a_batch, dtype=torch.float))\n",
    "            \n",
    "            # Create MiniBatch\n",
    "            #  mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\ #original Version, change to tensors at end\n",
    "            #              torch.tensor(r_batch, dtype=torch.float), torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "            #              torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
    "\n",
    "            #mini_batch = [s_batch, a_batch, r_batch, s_prime_batch, done_batch, prob_a_batch] #CKG - Version 1 (still working with lists)\n",
    "            \n",
    "            mini_batch = [s, a, r, s_prime, prob_a, done_mask] #HEAVILY Modified from original, but works?\n",
    "            data.append(mini_batch)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "                delta = td_target - self.v(s)\n",
    "            delta = delta.numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "\n",
    "        return data_with_adv\n",
    "\n",
    "\n",
    "    def train_net(self):\n",
    "        if len(self.data) == minibatch_size * buffer_size:\n",
    "            data = self.make_batch()\n",
    "            data = self.calc_advantage(data)\n",
    "\n",
    "            for i in range(K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "\n",
    "                    mu, std = self.pi(s, softmax_dim=1)\n",
    "                    dist = Normal(mu, std)\n",
    "                    log_prob = dist.log_prob(a)\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "                    loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.mean().backward() \n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimization_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Env if crashed\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "Couldn't launch the /Users/ckg-files/AgentWalker/CustomUnityEnvironments/Environment-1.app environment. Provided filename does not match any environments.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ckg/Desktop/AgentWalker/BaseModel.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.27/home/ckg/Desktop/AgentWalker/BaseModel.ipynb#ch0000008vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Run Above Model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.27/home/ckg/Desktop/AgentWalker/BaseModel.ipynb#ch0000008vscode-remote?line=1'>2</a>\u001b[0m unity_env \u001b[39m=\u001b[39m UnityEnvironment(\u001b[39m\"\u001b[39;49m\u001b[39m/Users/ckg-files/AgentWalker/CustomUnityEnvironments/Environment-1.app\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.27/home/ckg/Desktop/AgentWalker/BaseModel.ipynb#ch0000008vscode-remote?line=2'>3</a>\u001b[0m                              no_graphics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39m#no_graphics=True to run in headless mode\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.27/home/ckg/Desktop/AgentWalker/BaseModel.ipynb#ch0000008vscode-remote?line=3'>4</a>\u001b[0m env \u001b[39m=\u001b[39m UnityToGymWrapper(unity_env, uint8_visual\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, flatten_branched\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, allow_multiple_obs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m#Originally passed False ot allow_multiple_obs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.27/home/ckg/Desktop/AgentWalker/BaseModel.ipynb#ch0000008vscode-remote?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m PPO()\n",
      "File \u001b[0;32m~/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py:215\u001b[0m, in \u001b[0;36mUnityEnvironment.__init__\u001b[0;34m(self, file_name, worker_id, base_port, seed, no_graphics, timeout_wait, additional_args, side_channels, log_folder, num_areas)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py?line=212'>213</a>\u001b[0m \u001b[39mif\u001b[39;00m file_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py?line=213'>214</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py?line=214'>215</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process \u001b[39m=\u001b[39m env_utils\u001b[39m.\u001b[39;49mlaunch_executable(\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py?line=215'>216</a>\u001b[0m             file_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executable_args()\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py?line=216'>217</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py?line=217'>218</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m UnityEnvironmentException:\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/environment.py?line=218'>219</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py:101\u001b[0m, in \u001b[0;36mlaunch_executable\u001b[0;34m(file_name, args)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py?line=98'>99</a>\u001b[0m launch_string \u001b[39m=\u001b[39m validate_environment_path(file_name)\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py?line=99'>100</a>\u001b[0m \u001b[39mif\u001b[39;00m launch_string \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py?line=100'>101</a>\u001b[0m     \u001b[39mraise\u001b[39;00m UnityEnvironmentException(\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py?line=101'>102</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt launch the \u001b[39m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m environment. Provided filename does not match any environments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py?line=102'>103</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py?line=103'>104</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ckg/Desktop/AgentWalker/src/mlagents-envs/ml-agents-envs/mlagents_envs/env_utils.py?line=104'>105</a>\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe launch string is \u001b[39m\u001b[39m{\u001b[39;00mlaunch_string\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mUnityEnvironmentException\u001b[0m: Couldn't launch the /Users/ckg-files/AgentWalker/CustomUnityEnvironments/Environment-1.app environment. Provided filename does not match any environments."
     ]
    }
   ],
   "source": [
    "# Run Above Model\n",
    "unity_env = UnityEnvironment(CUSTOM_ENV_PATH, \n",
    "                             no_graphics=False) #no_graphics=True to run in headless mode\n",
    "env = UnityToGymWrapper(unity_env, uint8_visual=False, flatten_branched=False, allow_multiple_obs=True) #Originally passed False ot allow_multiple_obs\n",
    "model = PPO()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "mission_failed = 0 #keep track of number of failures\n",
    "rollout = []\n",
    "training_start_time = time.time()\n",
    "\n",
    "for n_epoch in range(1000):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        for t in range(rollout_len):\n",
    "            s = np.array(s) #was getting error as `s` was being returned as a list, not np array\n",
    "            mu, std = model.pi(torch.from_numpy(s).float())\n",
    "            dist = Normal(mu, std)\n",
    "            a = dist.sample()\n",
    "            log_prob = dist.log_prob(a)\n",
    "            #s_prime, r, done, info = env.step([a.item()]) #original\n",
    "            s_prime, r, done, info = env.step(a) #not a scalar anymore\n",
    "            s_prime = np.array(s_prime)\n",
    "\n",
    "            #print(\"s-\", s.shape)\n",
    "            #print(\"a-\", a.shape)\n",
    "            #print(\"r-\", r/10.0)\n",
    "            #print(\"s_prime-\", s_prime.shape)\n",
    "            #print(\"log_prob-\", log_prob.shape)\n",
    "            #print(\"done-\", done)\n",
    "            \n",
    "            #rollout.append((s, a.item(), r/10.0, s_prime, log_prob.item(), done)) #original\n",
    "            rollout.append((s, a, r/10.0, s_prime, log_prob, done))\n",
    "            if len(rollout) == rollout_len:\n",
    "                model.put_data(rollout)\n",
    "                rollout = []\n",
    "\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            if done:\n",
    "                mission_failed += 1 #we'll get them next time!\n",
    "                break\n",
    "\n",
    "        # If Model Stayed Up, Train!\n",
    "        model.train_net()\n",
    "\n",
    "        # Save Model if Score is Good\n",
    "\n",
    "    if n_epoch%print_interval==0 and n_epoch!=0:\n",
    "        #print(\"# of episode :{}, avg score : {:.1f}, opt step: {}\".format(n_epoch, score/print_interval, model.optimization_step)) #original training info print statement\n",
    "        \n",
    "        # Calc Time Elapsed\n",
    "        time_elapsed = time.time() - training_start_time\n",
    "        elapsed_time = 'Training Time: {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)\n",
    "        \n",
    "        # Print out Summary Training Information per Print Interval\n",
    "        print(f\"Epochs: {n_epoch} | Average Score: {score/print_interval:.2f} | Optimization Steps: {model.optimization_step} | Missions Failed: {mission_failed} | {elapsed_time}\")\n",
    "        score = 0.0 #reset score\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8dd99ed8a1d532a3eaa235eb969059a5e42ed11e738e24ff08f1c639807b0204"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('walker')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
